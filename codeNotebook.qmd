---
title: "Assignment 2"
author: "C. Twaddle, L. De Jong and D Grassman"
format:
  pdf:
    toc: false
    number-sections: true
    colorlinks: true
    geometry:
      - top=22.4mm
      - left=22.4mm
      - right=22.4mm
      - bottom=22.4mm
---

```{r}
#| echo: false
#| message: false
library(tidyverse)
library(lubridate)
library(ggplot2)
library(caret)
library(factoextra)
library(cluster)
library(zoo)
library(janitor)
library(nnet)  
library(mlogit)
library(randomForest)
library(hrbrthemes)
library(factoextra)
library(treemapify)
```

# Data Wrangling

-   External population data sourced from: https://data.openup.org.za/dataset/police-district-population-mwzd-ptz7

```{r}
#| warning: false
#| message: false
pop_data <- read.csv("data/estimated_population.csv") |> 
  mutate(Station = str_to_title(precinct)) |> 
  select(Station, estimated_population, estimated_male, estimated_female)

crime_data <- read.csv("data/South Africa crime data.csv") |> 
  pivot_longer(cols = c(-Province, -Station, -Category), 
               names_to = "Year", 
               values_to = "Count" ) |> 
  mutate(Year = gsub("X|\\..*", "", Year)) |> 
  mutate(Year = year(as.Date(paste0(Year, '-01-01'), format ="%Y"))) |> 
  left_join(pop_data, by="Station") 
  
so_data <- crime_data |> 
  filter(Category == "Sexual Offences" & Count != 0) |> 
  pivot_wider(names_from = "Category", values_from = Count) |> 
  rename(sexual_offences = "Sexual Offences") |> 
  select(Year, Province, Station, sexual_offences, 
         estimated_population, estimated_male, estimated_female) |>
  na.omit()

crime_rate <- crime_data |>
  group_by(Year) |>
  summarise(total=sum(Count))|>
  arrange(desc(Year))

province_crime <- crime_data |>
  group_by(Province) |>
  summarise(total=sum(Count))|>
  arrange(desc(total))
```

# Background

This section aims to provide insight into the state of crime in South African for a 10 year period (2005 - 2015). It will include various visualisations to illustrate the statistics of interest.

### Total crimes per province

```{r fig.width=6, fig.height=4}
#| warning: false
#| message: false

ggplot(data = crime_rate, aes(x = Year, y = total))+
  geom_point(color = "#003249")+
  geom_line(color = "#00699A")+
  scale_x_continuous(breaks = unique(crime_rate$Year)) +
  scale_y_continuous(labels = scales::comma) +
  labs(x = "Year", 
       y = "Total Crimes Reported", 
       title = "Overall crime rate in South Africa")+
  theme_ipsum(base_family = "sans")+
  theme(axis.title.x = element_text(size = 13, hjust = 0.5),
        axis.title.y = element_text(size = 13, hjust = 0.5),
        text = element_text(size = 7))

```

The graph above shows the overall crime rate for the whole country of South Africa over the 10 years. We can see that while the line does dip once or twice, the crime rate has increased over the period.

## Total crimes per province

```{r fig.height=6}
#| warning: false
#| message: false

ggplot(data = province_crime, 
       aes(x = reorder(Province, total) , y = total)) +
  geom_bar(stat = "identity", width = 0.5, fill = "#00699A") +
  geom_text(aes(label = format(round(total,2), big.mark = ",", 
                               decimal.mark = ".")), vjust=-0.5, size = 3, 
            fontface = "bold",)+
  scale_y_continuous(labels = scales::comma) +
  ylim(0, 8000000)+
  labs(
    x = "Province",
    y = "Total Crimes",
    title = "Total Crimes per Province")+
  theme_ipsum(base_family = "sans")+
  theme(axis.title.x = element_text(size = 13, hjust = 0.5),
        axis.title.y = element_text(size = 13, hjust = 0.5),
        axis.text.x = element_text(angle = 90),
        axis.text.y = element_blank())
```

The above graph represents the total cumulative crime count per province. Gauteng has the highest crime rate, which will influence our choice of sampling the dataset in later models.

### Areas that experience higher crime per province

```{r}
#| warning: false
#| message: false

crime_region<- crime_data |>
  group_by(Province, Station) |>
  summarise(total=sum(Count)) |>
  arrange(desc(total)) |>
  slice_head(n = 10)
crime_region
```

This table shows the top 10 stations with the highest cumulative crime count per province. \## #Most common crimes in South Africa

```{r}
#| warning: false
#| message: false

crime_type<- crime_data |>
  group_by(Category) |>
  summarise(total=sum(Count)) |>
  arrange(desc(total))
crime_type

```

This table ranks the crime types from the highest cumulative count to the lowest.

```{r fig.width=7}
#| warning: false
#| message: false

data <- crime_data |> 
  summarise(total = sum(Count))
crime_type<- crime_data |>
  group_by(Category) |>
  summarise(total=sum(Count)) |>
  arrange(desc(total))
treemap_data <- crime_type |>
  mutate(total_percent = (total / data$total)*100) |> 
  slice_head(n = 10)

pal <- c("#bc4749", "#005f73", 
         "#0a9396", "#94d2bd", 
         "#bb3e03","#003049",
         "#fca311", "#6FA928",
         "#9a5059", "#F0781C")

treemap_graph <- treemap_data|>
  ggplot(aes(area = total_percent, fill = Category, 
             label = paste0(Category, "\n", sprintf("%.2f%%", total_percent)))) +
  geom_treemap() +
  geom_treemap_text(place = "centre", colour = "white", size = 15) +
  labs(title = "Top 10 Crimes by Percentage") +
  scale_fill_manual(values = pal)+
  theme(legend.position = "none")
treemap_graph
```

The above tree map displays the most often occurring crimes by percentage.

# Regression

Our goal with our regression model is to investigate whether there is a strong correlation between the number of males in the population and the amount of sexual offences (so) that are reported. We aim to test if the model could predict the number of so crimes that will be reported. Please note that the population estimates were sourced from the 2011 South African Census, and the regression model will therefore be limited to the year 2011.

## EDA

Going forward our hypotheses are as follows: H0: The number of males in a population has no influence on the number of sexual offences reported. HA: The number of males in a population has an influence on the number of sexual offences reported.

Our first step into exploring the data is to test for correlation.

```{r}
#| warning: false
#| message: false
cor(so_data$sexual_offences, so_data$estimated_male)
```

The above result shows that the two variables of interest, number of males in population and number of reported sexual offences, is highly positively correlated. The following function gives more insight into this correlation.

```{r}
#| warning: false
#| message: false
cor.test(so_data$sexual_offences, so_data$estimated_male)
```

In the following output shows a t-value of 130,62. This value indicates how far the correlation coefficients are from zero in standard errors. In our case, the t-value shows that the correlation coefficients are quite far from zero and a larger t-value provides strong evidence to support the alternative hypothesis. The p-value is extremely small, much less than 0,05, which once again supports the alternative hypothesis.

Below are some graphs to visualise these findings.

```{r}
#| warning: false
#| message: false

so_graph_whole_pop <- so_data |> 
  filter(Year == 2011) |> 
  group_by(Province) |> 
  ggplot(aes(x=estimated_population, 
             y=sexual_offences, 
             colour = Province))+
  geom_point()+
  geom_smooth()+
  scale_x_continuous(labels = scales::comma)+
  labs(x="Total Population",
       y="Number of Sexual Offences")+
  theme_ipsum(base_family = "sans")+
  theme(axis.title.x = element_text(size = 13, hjust = 0.5),
        axis.title.y = element_text(size = 13, hjust = 0.5),
        text = element_text(size = 8))
so_graph_whole_pop
```

The above graph shows the correlation between the number of sexual offences reported against the total population, split by province. We can see a common upward trend in most provinces.

```{r}
#| warning: false
#| message: false

so_graph_one_line <- so_data |> 
  filter(Year == 2011) |> 
  group_by(Province) |> 
  ggplot(aes(x=estimated_male, 
             y=sexual_offences))+
  geom_point(aes(color=Province))+
  geom_smooth(color="#00699A")+
  scale_x_continuous(labels = scales::comma) +
  labs(x="Male Population",
       y="Number of Sexual Offences")+
  theme_ipsum(base_family = "sans")+
  theme(axis.title.x = element_text(size = 13, hjust = 0.5),
        axis.title.y = element_text(size = 13, hjust = 0.5),
        text = element_text(size = 8))
so_graph_one_line
```

This graph indicates the correlation between sexual offences and the male population for the country as a whole. This graph supports the statistics that were explored above.

```{r}
#| warning: false
#| message: false

so_graph_per_province <- so_data |> 
  filter(Year == 2011) |> 
  group_by(Province) |> 
  ggplot(aes(x=estimated_male, 
             y=sexual_offences, 
             color=Province))+
  geom_point()+
  geom_smooth()+
  scale_x_continuous(labels = scales::comma)+
  labs(x="Male Population",
       y="Number of Sexual Offences")+
  theme_ipsum(base_family = "sans")+
  theme(axis.title.x = element_text(size = 13, hjust = 0.5),
        axis.title.y = element_text(size = 13, hjust = 0.5),
        text = element_text(size = 8))
so_graph_per_province
```

This graph expands on both previous graphs by exploring the relationships among the all the provinces to see if this is a consistent trend. We can see that most provinces follow a general upward trajectory of sexual offences as male population increases, except for Mpumalanga. This variance will be investigated further.

## Models

The first model created is a simple, two variable linear regression.

```{r}
#| warning: false
#| message: false
summary(lm(sexual_offences ~ estimated_male, data = so_data))
```

This summary shows us that the models sees these variables as also having a strong positive correlation as the p-value is less than 0,05. The residual standard error (RSE) suggest that on average the predicted values differ from the observed values at a rate of 41,36 units. Multiple R-squared shows that 61,86% of the predicted variables are explained by the predictors of the model. The F-statistics is the result of an f-test, and shows the overall significance of the model. It tries to explain the variability of the model. Our f-statistic is quite large and therefore the model can handle big variability.

```{r}
#| warning: false
#| message: false

summary(aov(sexual_offences ~ estimated_male, data = so_data))
so_data |>
  rstatix::anova_test(sexual_offences ~ estimated_male)
```

The above model is a simplified analysis of variance, which also uses a linear method, and confirms the linear regressions findings.

```{r}
#| warning: false
#| message: false

test <-aov(sexual_offences ~ estimated_male * Province, data = so_data)
summary(test)
```

The above model has been expanded to investigate whether the province has any significance in our investigation. The model indicates that province as a factor may be of importance.

### Machine learning model

The following is a linear regression machine learning model that was trained on a subset of data and then tested on the rest.

```{r}
#| warning: false
#| message: false
set.seed(177) #for reproducibility

index <- createDataPartition(so_data$sexual_offences, p =0.8, list = FALSE) 
#creates a subset of data 

training <- so_data[index, ]
testing <- so_data[-index, ]

model <- lm(sexual_offences ~ estimated_male, data = training) 

predictions <- predict(model, newdata = testing) #test model

ggplot(testing, aes(x=sexual_offences, 
                    y=predictions))+
  geom_point(color="#00699A")+
  geom_smooth(color = "#FF674D") + 
  labs(x="Actual",
       y="Predicted") + 
  ggtitle("Actual vs Predicted") +
  theme_ipsum(base_family = "sans")+
  theme(axis.title.x = element_text(size = 13, hjust = 0.5),
        axis.title.y = element_text(size = 13, hjust = 0.5),
        text = element_text(size = 8))
```

The above graph plots the predicted outcomes of the model versus the actual values from the training data. We can see from this graph that the model is relatively accurate the smaller the variables, but gets less accurate as the variables increase. This could be explained by the fact that most of the data lies closer to the origin so training for larger values is difficult.

```{r}
summary(model)
```

The summary of the model and the graph of the results show conflicting results. The table shows that the model is fairly accurate and the variables are strongly correlated, however the graph shows that the outcomes of the model get less accurate. This difference in evaluation of the model could be explained by coincidental correlation. As population in general increases it is expected that crime in general will also increase. Since an increase in males in a population still increases the population overall, the result of increased sexual offences is expected since overall crime will increase. However this exploration was still insightful into whether there is an effect at all.

# Clustering

Since clustering is an unsupervised machine learning technique, we do not have any expectations for the outcome of our exploration.

## EDA

We gave the model a subset of data that included station names, categories of crime, and the number of reports per crime time. Our aim was for the model to cluster stations based on either crime type or crime frequency.

Our first step into exploring the data is to clean and cut the data. We chose to select a random sample of stations from each province to limit the messiness of out EDA and to test if the model could create cohesive clusters. We once again reduced the data set to 2011. We did this to avoid problems in the modelling and chose 2011 to be consistent with our regression model.

```{r}
#| warning: false
#| message: false

set.seed(177) #for reproducibility 

crime_sample <- crime_data |>
  group_by(Province, Station) |>
  summarise(total=sum(Count)) |>
  arrange(desc(total)) |>
  sample_n(7) #selects a random 7 stations from each province

#creates a list of the randomly selected stations
list <- as.list(unique(crime_sample$Station))  

#remove bank robbery since it is too insignificant and 
#skews the outcome
crime_data_clust <- crime_data |> 
  filter(Year == 2011) |> 
  filter(Category != "Bank robbery") |> 
  filter(Count != 0) |> 
  filter(Station %in% list) |> 
  select(Station, Category, Count) |> 
  pivot_wider(names_from = Category, 
              values_from = Count) |> 
  column_to_rownames(var = "Station") 
#converts the station names to the index

#replaces NAs with averages
crime_data_clust <- na.aggregate(crime_data_clust) 
```

Our next step is to create and visualise graphs to help build the clustering model.

```{r}
#| warning: false
#| message: false
set.seed(177)
distance <- get_dist(crime_data_clust, method = "euclidean")

fviz_dist(distance, gradient = list(low = "#00699A", 
                                    mid = "white", 
                                    high = "#FF674D"))
```

The above matrix plots the different station against each other. The darker colour represents stations with higher correlations. This could indicate that these stations have similar crime patterns.

We ran multiple tests to determine the optimal number of clusters.

```{r}
#| warning: false
#| message: false
set.seed(177)
fviz_nbclust(x=crime_data_clust , kmeans, method=c("wss")) +
  scale_y_continuous(labels = scales::comma)+
  labs(y="")+
  theme_ipsum(base_family = "sans")+
  theme(axis.title.x = element_text(size = 13, hjust = 0.5),
        axis.title.y = element_text(size = 13, hjust = 0.5),
        text = element_text(size = 8))
```

The above graph uses the data given and the 'elbow' method to guess the optimal number of clusters. This graph seems to indicate that the optimal clusters lies at either 2 or 3.

```{r}
#| warning: false
#| message: false
set.seed(177)
fviz_nbclust(x=crime_data_clust , kmeans, method = c("silhouette")) +
  scale_y_continuous(labels = scales::comma)+
  labs(y="")+
  theme_ipsum(base_family = "sans")+
  theme(axis.title.x = element_text(size = 13, hjust = 0.5),
        axis.title.y = element_text(size = 13, hjust = 0.5),
        text = element_text(size = 8))
```

The above graph uses the data given and the silhouette method to guess the optimal number of clusters. This graph indicates that the optimal clusters is 2.

```{r}
#| warning: false
#| message: false
set.seed(177)
gap_stat <- clusGap(crime_data_clust , FUN = kmeans, nstart = 25, K.max = 10, B=50) #compute gap statistic

print(gap_stat, method = "firstmax")
```

```{r}
#| warning: false
#| message: false
set.seed(177)
fviz_gap_stat(gap_stat) + 
  theme_ipsum(base_family = "sans")+
  theme(axis.title.x = element_text(size = 13, hjust = 0.5),
        axis.title.y = element_text(size = 13, hjust = 0.5),
        text = element_text(size = 8))
```

The above table and graph uses the gap statistic method to guess the optimal number of clusters. These outputs indicate that the optimal clusters is 7.

## Model

The following code creates and visualises the clustering model using k-means methods. We decided to set 3 clusters based on the on the previous cluster graphs.

```{r fig.height=5}
#| warning: false
#| message: false
set.seed(177)

final <- kmeans(crime_data_clust, 3, nstart = 25) 

fviz_cluster(final, data=crime_data_clust, 
             palette = c("#464D77", "#36827F", "#F9DB6D"))+
  theme_ipsum(base_family = "sans",)+
  theme(axis.title.x = element_text(size = 13, hjust = 0.5),
        axis.title.y = element_text(size = 13, hjust = 0.5),
        text = element_text(size = 8))
```

The graph above shows the outcome of the k-means clustering model. We can see three clearly defined clusters. Each cluster groups a number of stations together.

The following table aims to provide insight into the clusters created from the sample data and why certain stations may have been grouped together. It shows the average of each crime time for each cluster.

```{r}
#| warning: false
#| message: false
set.seed(177)

clusters_table <- crime_data_clust |> 
  mutate(Cluster = final$cluster) |> 
  group_by(Cluster) |> 
  summarise_all("mean")
clusters_table
```

Cluster 1: This cluster has high averages of the following crimes, All theft not mentioned elsewhere, Burglary at residential premises and Theft out of or from motor vehicle. This seems to shows stations in this cluster experience high levels of different types of theft and burglary.

Cluster 2: This seems to experience most crimes at a median average compared to the other clusters.

Cluster 3: This cluster has the lowest overall average crime across categories.

# Classification

Our goal with our classification model is to investigate whether the model could correctly classify each station into a certain risk category. With this model, we wanted the type of crime to contribute to the crime level, so that an area with 5 murders would not be classified as "less dangerous" than an area with 10 shoplifting reports. In order to take this into account, we consulted SAPS website and classified the crimes into broader categories and assigned each broad category a weight. The process of adding this to our dataset is as follows:

```{r}
#| warning: false
#| message: false
# creates crime types for classification
crime_types <- data_frame (Contact_Crimes_against_an_individual = 
                             c("Murder", "Sexual Offences", "Attempted murder",
                        "Assault with the intent to inflict grievous bodily harm",
                               "Robbery with aggravating circumstances",
                               NA),
                           Low_Contact_Crimes = 
                             c("Common assault", "Common robbery",
                               "Arson", NA, NA, NA),
                           Property_related_Crimes = 
                             c("Malicious damage to property", 
                               "Burglary at non-residential premises",
                               "Burglary at residential premises",
                               "Theft of motor vehicle and motorcycle",
                               "Theft out of or from motor vehicle",
                               "Stock-theft"),
                           Aggravated_Robbery = 
                             c("Carjacking", 
                             "Robbery at non-residential premises", 
                             "Robbery at residential premises", 
                             "Robbery of cash in transit",
                              "Bank robbery", "Truck hijacking"),
                           Crimes_detected_due_to_Police_Action = 
                             c("Illegal possession of firearms and ammunition",
                               "Drug-related crime",
                               "Driving under the influence of alcohol or drugs",
                               "Sexual offences as a result of police action", NA,
                               NA),
                           Misdemeanor = c("All theft not mentioned elsewhere",
                                           "Commercial crime", "Shoplifting", 
                                           NA, NA, NA))

crime_types_long <- pivot_longer(crime_types, 
                           cols = everything(), 
                           names_to = "crime_category", 
                           values_to = "Crime_Type") |>
  na.omit()

#assigns each crime type a weight
crime_types_long <- crime_types_long |>
  mutate(
  crime_weight = case_when(
    crime_category == "Contact_Crimes_against_an_individual" ~ 1.9 ,
    crime_category == "Low_Contact_Crimes" ~ 1.8 , 
    crime_category == "Property_related_Crimes" ~ 1.7 , 
    crime_category == "Aggravated_Robbery" ~ 1.6 , 
    crime_category == "Crimes_detected_due_to_Police_Action" ~ 1.5 , 
    crime_category == "Misdemeanor" ~ 1.4 ) )
```

## EDA

In order to visual the changes we made in a easy to read graph, we decided to cut our dataset down to the province of Gauteng in 2011. This made the data small enough to read the graph, but still include stations in diverse areas. In this chunk we also computed the weighted count of the crimes.

```{r}
#| warning: false
#| message: false

#reduce dataset
crime_data_jhb <- crime_data |> 
  left_join(crime_types_long, by = c("Category" = "Crime_Type")) |> 
  #creates new weighted count column
  mutate(weighted_count = Count * crime_weight) |> 
  filter(Year == 2011) |> 
  filter(Province == "Gauteng") |> 
  select(Station, Category, Count, crime_weight, weighted_count,
         estimated_population)
```

When we first explored this idea, we wanted to take district population into account. We computed extra columns calculating the crime per thousand people, as well as the weighted crime per thousand people.

```{r}
#| warning: false
#| message: false

crime_data_pop_jhb <- crime_data_jhb |>
  group_by(Station) |>
  filter(weighted_count != 0) |> 
  mutate(total_crime =  sum(Count)) |> 
  mutate(total_weighted_crime = sum(weighted_count)) |> 
  mutate(crime_per_thousand = (total_weighted_crime/estimated_population)*1000)|> 
  mutate(unw_crime_per_thou = (total_crime/estimated_population)*1000) |> 
  arrange(desc(total_weighted_crime))
```

```{r fig.height=6, fig.width=7}
#| warning: false
#| message: false

crime_data_pop_jhb |> 
  ggplot(aes(x=reorder(Station,total_crime), y=total_crime)) +
  geom_bar(stat = "identity", fill="#00699A") +
  scale_y_continuous(labels = scales::comma)+
  scale_x_discrete()+
  labs(x = "Station Names",
       y = "Total Crime",
       title = "Total Crime Count per Station in Gauteng")+
  theme_ipsum(base_family = "sans",)+
  theme(axis.title.x = element_text(size = 13, hjust = 0.5),
        axis.title.y = element_text(size = 13, hjust = 0.5),
        axis.text.x = element_text(angle = 90, vjust= 0.5, hjust = 1),
        text = element_text(size = 5))
```

The graph above shows the total crime count per district in Gauteng. The graph is ordered from lowest crime areas to highest crime areas. This shows that Gauteng has areas of diverse crime levels.

```{r fig.height=6,fig.width=7}
#| warning: false
#| message: false

crime_data_pop_jhb |> 
  na.omit() |> 
  ggplot(aes(x=reorder(Station,crime_per_thousand), y=crime_per_thousand)) +
  geom_bar(stat = "identity", fill="#00699A") +
  scale_y_continuous(labels = scales::comma)+
  scale_x_discrete()+
  labs(x = "Station Names",
       y = "Crime per Thousand",
       title = "Total Crime Count per Thousand People per Station in Gauteng")+
  theme_ipsum(base_family = "sans",)+
  theme(axis.title.x = element_text(size = 13, hjust = 0.5),
        axis.title.y = element_text(size = 13, hjust = 0.5),
        axis.text.x = element_text(angle = 90, vjust= 0.5, hjust = 1),
        text = element_text(size = 5))
```

The graph above plots the total weighted crime per thousand people per district. As we can see, there is one station that is heavily skewing the data. This is OR Tambo International Airport police station. This high weighted crime rate is due to the fact that many crimes are reported to that station, despite the fact that the actual district population is very low (Around 170 as of the 2011 Census).

```{r fig.height=6, fig.width=7}
crime_data_pop_jhb |> 
  na.omit() |> 
  filter(Station != "Or Tambo Intern Airp") |> 
  ggplot(aes(x=reorder(Station,unw_crime_per_thou), y=unw_crime_per_thou)) +
  geom_bar(stat = "identity", fill="#00699A") +
  scale_y_continuous(labels = scales::comma)+
  scale_x_discrete()+
  labs(x = "Station Names",
       y = "Crime per Thousand",
       title = "Total Crime Count per Thousand People per Station in Gauteng",
       subtitle = "Excluding OR Tambo International Airport Station")+
  theme_ipsum(base_family = "sans",)+
  theme(axis.title.x = element_text(size = 13, hjust = 0.5),
        axis.title.y = element_text(size = 13, hjust = 0.5),
        axis.text.x = element_text(angle = 90, vjust= 0.5, hjust = 1),
        text = element_text(size = 5))
```

For the graph above, we removed OR Tambo Airport station to show just how skewed the data had become. We also noticed that overall, the effect that population was having was not reflecting the true status of each district, and therefore have not taken it into account further.

Below we increased the dataset to include every province in the year 2011, and this is the data we used to train our model.

```{r}
#| warning: false
#| message: false

crime_data_weight <- crime_data |> 
  left_join(crime_types_long, by = c("Category" = "Crime_Type")) |> 
  filter(Year == 2011) |> 
  mutate(weighted_count = Count * crime_weight) |> 
  select(Station, Category, Count, crime_weight, weighted_count, estimated_population) 
```

```{r fig.height=6, fig.width=7}
crime_data_weight_pop <- crime_data_weight |>
  group_by(Station) |>
  filter(weighted_count != 0) |> 
  mutate(total_crime =  sum(Count)) |> 
  mutate(total_weighted_crime = sum(weighted_count)) |> 
  arrange(desc(total_weighted_crime)) 

crime_data_weight_pop |> 
  ggplot(aes(x=reorder(Station,total_weighted_crime), y=total_weighted_crime)) +
  geom_bar(stat = "identity", fill="#00699A") +
  scale_y_continuous(labels = scales::comma)+
  scale_x_discrete()+
  labs(x = "Station Names",
       y = "Total Crime",
       title = "Total Weighted Crime Count per Station")+
  theme_ipsum(base_family = "sans",)+
  theme(axis.title.x = element_text(size = 13, hjust = 0.5),
        axis.title.y = element_text(size = 13, hjust = 0.5),
        axis.text.x = element_text(angle = 90, vjust= 0.5, hjust = 1),
        text = element_text(size = 5))
```

The above graph shows the total weighted crime for all stations in the year 2011. This graph helps to visualise the areas that would be high risk, medium risk and low risk.

We decided to use the interquartile range to classify the risk levels of each station. Stations with a weighted crime count of greater than the 75% quartile were classified as high risk. Stations with a weighted crime count of less than the 25% quartile were classified as low risk. Every other station was classified as moderate risk.

```{r}
#compute quartiles
quartiles <- quantile(na.omit(crime_data_weight_pop$weighted_count), 
                      probs = c(0.25, 0.5, 0.75))
top_quart <- max(quartiles)
low_quart <- min(quartiles)

#Assign a risk level based on total weighted
crime_data_levels <- crime_data_weight_pop |> 
 mutate(
  danger_level = case_when(
    weighted_count >= top_quart ~ "High risk",
    weighted_count <= low_quart ~ "Low risk",
    .default = "Moderate risk"
    )) |> 
  na.omit() |> 
  select(c(-Count, -crime_weight, -estimated_population, -total_crime,
           -total_weighted_crime)) |>  #remove columns not needed for the model
  pivot_wider(names_from = Category,
              values_from = weighted_count) |>
  clean_names() |> 
  mutate_at(vars(-station), ~ replace_na(., 0)) #replace all NAs with 0
```

The dataset is split into 70% train data, and 30% testing data. We used a randome forest classification model since we have 3 categories for it to classify stations into.

```{r}
set.seed(177)

#turn categories into factors
crime_data_levels$danger_level <- factor(crime_data_levels$danger_level,
                                         levels = c("Low risk", "Moderate risk",
                                                    "High risk")) 

#split training and testing data
splitIndex <- createDataPartition(crime_data_levels$danger_level,
                                  p = 0.7, list = F) 
train <- crime_data_levels[splitIndex, ]
test <- crime_data_levels[-splitIndex, ]

rfGrid <- expand.grid(mtry = c(1,2,3)) 

fitControl <- trainControl(method = "cv", number = 5) #cross validate data

#predict danger/risk level based on weighted count of all types of crime
#(could not remove station as a column so the crimes are all typed out)
class_model <- train(danger_level ~ all_theft_not_mentioned_elsewhere +
                       commercial_crime + robbery_with_aggravating_circumstances +
                       shoplifting + common_robbery +
                       theft_of_motor_vehicle_and_motorcycle + common_assault +
                       assault_with_the_intent_to_inflict_grievous_bodily_harm +
                       theft_out_of_or_from_motor_vehicle +
                       burglary_at_non_residential_premises +
                       malicious_damage_to_property +
                       robbery_at_non_residential_premises + sexual_offences +
                       burglary_at_residential_premises + drug_related_crime +
                       carjacking + 
                       driving_under_the_influence_of_alcohol_or_drugs +
                       attempted_murder + murder + robbery_at_residential_premises+
                       arson + robbery_of_cash_in_transit + truck_hijacking +
                       bank_robbery + stock_theft,
                data = train,
                method = "rf",
                ntree = 1000,
                trControl = fitControl,
                tuneGrid = rfGrid)
```

We then give the model the testing data for it to make predictions.

```{r}
predictions <- predict(class_model, newdata = test)
```

```{r}
cm <- confusionMatrix(predictions, test$danger_level)
cm
```

The above output shows the important information about our model. It is 99.62% accurate and has a high confidence interval.

```{r}
conf_matrix <- as.data.frame(cm$table)
ggplot(conf_matrix, aes(x = Prediction, y = Reference, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), vjust = 1) +
  scale_fill_gradient(low = "#F0FFFC", high = "#00699A") +
  labs(x = "Predicted Danger Level", 
       y = "Actual Danger Level", 
       title = "Confusion Matrix") +
  theme_minimal()
```

The above plot is a visual representation of our confusion matrix.

## Extra test

In order to see how well our model handles a bigger dataset, we tested it on every year except for 2011 (which it was partially trained on). The increase in data only reduced the model accuracy by 0.6%.

```{r}
crime_test <- crime_data |> 
  left_join(crime_types_long, by = c("Category" = "Crime_Type")) |> 
  filter(Year != 2011) |> 
  mutate(weighted_count = Count * crime_weight) |> 
  select(Year, Station, Category, Count, crime_weight, weighted_count)
crime_test <- crime_test |> 
  group_by(Station) |>
  filter(weighted_count != 0) |> 
  mutate(total_crime =  sum(Count)) |> 
  mutate(total_weighted_crime = sum(weighted_count)) |> 
  arrange(desc(total_weighted_crime)) 
crime_test <- crime_test |> 
   mutate(
  danger_level = case_when(
    weighted_count >= top_quart ~ "High risk",
    weighted_count <= low_quart ~ "Low risk",
    .default = "Moderate risk"
    )) |> 
  select(c(-Count, -crime_weight, -total_crime, -total_weighted_crime)) |>  
  pivot_wider(names_from = Category,
              values_from = weighted_count) |>
  clean_names() |> 
  mutate_at(vars(-station), ~ replace_na(., 0))

crime_test$danger_level <- factor(crime_test$danger_level, 
                                  levels = c("Low risk", "Moderate risk", 
                                             "High risk"))

second_predictions <- predict(class_model, newdata = crime_test)
```

```{r}
cm2 <- confusionMatrix(second_predictions, crime_test$danger_level)
cm2
```

```{r}
conf_matrix <- as.data.frame(cm2$table)
ggplot(conf_matrix, aes(x = Prediction, y = Reference, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), vjust = 1) +
  scale_fill_gradient(low = "#F0FFFC", high = "#00699A") +
  labs(x = "Predicted Danger Level", 
       y = "Actual Danger Level", 
       title = "Confusion Matrix") +
  theme_minimal()
```

# Recommendations

Based on our background and findings we present the following recommendations.

Despite the mixed results of our regression model, gender based violence is a prevalent issue in South Africa. Initiatives should be started to help victims and make them feel safe to come forward for help. Every police station should have a representative able to deal with gender based violence and sexual offences, and collaborate with NGOs that aim to support victims.

Each station should be aware of what type of crime is most prevalent in their district and develop strategies for managing those crimes. For example, areas that our model assigned to cluster 1 should be aware of the heightened prevalence of theft and implement a prevention strategy, such as more frequent patrols around residential areas.

Our classification model is an example of something that be implemented by SAPS to keep track of which areas are at what risk level. If SAPS implements a better, automated system it could automatically assign, weight and track all the reports that are made at a station. It could then update the level of that areas based on current statistics in real time.

Better records of the results of cases should be kept. It was not possible to find data on the resolution of reports that were made, and that type of insight could be useful in identifying areas where the SAPS system is lacking.
